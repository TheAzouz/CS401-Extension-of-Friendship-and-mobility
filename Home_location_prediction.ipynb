{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "In this notebook, we will try to create features from checkin dataset.\n",
    "We based our work on the paper: `Fine-Scale Prediction of People’s Home Location Using Social Media Footprints` by H. Kavak et al. The authors start by clustring the checkins of each user using Density-based spatial clustering of applications with noise (DBSCAN). The goal of this step is to group checkins into dense and small clusters. One of these clusters contain the home location. To predict the latter, they generated the following mobility features per cluster:\n",
    "\n",
    "- Check-in Ratio (CR)\n",
    "- Check-in Ration during Midnight (MR)\n",
    "- Check-in Ratio of Last Destination of a Day (EDR)\n",
    "- Check-in Ratio of Last Destination of a Day with Inactive Midnight (EIDR)\n",
    "- PageRank (PR)\n",
    "- Reverse PageRank (RPR)\n",
    "\n",
    "We will explain each feature as we progress in this work.\n",
    "\n",
    "\n",
    "These feature will be used to classify each cluster and we average the latitude and longitude to obtain users' home location\n",
    "\n",
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# functions to compute the distance between two geocoordinates\n",
    "from haversine import haversine_vector, Unit\n",
    "# DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "# To do operations on datetime\n",
    "from datetime import timedelta\n",
    "# PageRank and ReversePageRank\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Latitude correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_latitude(lat):\n",
    "    \"\"\"\n",
    "    This function corrects for out of range latitude.\n",
    "    \n",
    "    Input: \n",
    "    -- lat: latitude coordinates in °\n",
    "    Output: \n",
    "    -- lat: latitude coordinates put between -90 and 90°\n",
    "    \"\"\"\n",
    "    while lat>90 or lat<-90:\n",
    "        if lat>90:\n",
    "            lat = -(lat-180)\n",
    "        elif lat<-90:\n",
    "            lat = -(lat+180)\n",
    "    return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_longitude(long):\n",
    "    \"\"\"\n",
    "    This function corrects for out of range longitude.\n",
    "    \n",
    "    Input: \n",
    "    -- long: longitude coordiantes in °\n",
    "    Output: \n",
    "    -- long: longitude coordinates put between -180 and 180°\n",
    "    \"\"\"\n",
    "    while long>180 or long<-180:\n",
    "        if long>180:\n",
    "            long = long - 360\n",
    "        elif long<-180:\n",
    "            long = long +360\n",
    "    return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(df,columns):\n",
    "    '''\n",
    "    This function computes the distance between two geographic coordinates for a given dataframe.\n",
    "    \n",
    "    Input: \n",
    "        - df: Dataframe containing 4 columns latitude1, longitude1, latitude2 and longitude2\n",
    "        - columns: list of columns [latitude1, longitude1, latitude2 and longitude2]\n",
    "        \n",
    "    Output: \n",
    "        - numpy array containing the distance between geographic coordinates of each row\n",
    "    '''\n",
    "    points1 = list(zip(df[columns[0]],df[columns[1]]))\n",
    "    points2 = list(zip(df[columns[2]],df[columns[3]]))\n",
    "    # Use harvesine_vector to compute the distance between points\n",
    "    return np.round(haversine_vector(points1,points2,Unit.KILOMETERS),decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_relevant_homes(df_homes):\n",
    "    '''\n",
    "    This function selects relevant homes. We consider a home location as relevant if it's latitude and\n",
    "    longitude doesn't \"vary much\". To measure this variation, we simply compute the mean and the std of\n",
    "    the latitude and longitude of homes for every user. Then we construct 4 points as follow:\n",
    "        - by adding and substracting the standard deviation of the latitude and longitude from their\n",
    "        respective mean\n",
    "        - Measure the diagonal in KM\n",
    "        - If the diagonal is less than 100m we can assume with confidence that the mean is indeed the\n",
    "        home location\n",
    "    \n",
    "    Input:\n",
    "        - df_homes: A dataframe containing all checkins labled as Home\n",
    "    Output:\n",
    "        - df_homes: Home location for each user\n",
    "    '''\n",
    "    \n",
    "    # Grouping df_homes according to the user id and compute std and mean for lat and lon\n",
    "    df_homes = df_homes.groupby('User_ID').agg({'lat':('std','mean'),'lon':('std','mean')})\n",
    "    \n",
    "    # Filling nan values with 0 (std return 0 if there is only one sample)\n",
    "    df_homes.fillna(0,inplace = True)\n",
    "    \n",
    "    # Construct the diagonal points\n",
    "    df_tmp = pd.DataFrame()\n",
    "    df_tmp['lat1'] = df_homes.lat['mean']-df_homes.lat['std']\n",
    "    df_tmp['lat2'] = df_homes.lat['mean']+df_homes.lat['std']\n",
    "    df_tmp['lon1'] = df_homes.lon['mean']-df_homes.lon['std']\n",
    "    df_tmp['lon2'] = df_homes.lon['mean']+df_homes.lon['std']\n",
    "    \n",
    "    # Compute diagonal length\n",
    "    df_tmp['home_radius'] = compute_distance(df_tmp,['lat1','lon1','lat2','lon2'])\n",
    "    \n",
    "    # Filter home and keep relevant home (estimated distance between homes checkins < 100m )\n",
    "    df_homes = df_homes[df_tmp['home_radius']<0.1][[('lat','mean'),('lon','mean')]].copy()\n",
    "    \n",
    "    # Flatten df_homes columns\n",
    "    df_homes.columns = df_homes.columns.get_level_values(0)\n",
    "    \n",
    "    return df_homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_df_checkins(path,sample_frac = 1):\n",
    "    '''\n",
    "    This function takes the path of the raw data, import it and construct a checkin dataframe where\n",
    "    all users have at least 5 checkins and 1 home location\n",
    "    \n",
    "    Input:\n",
    "        - Path: the Path of the file containing the data\n",
    "        - sample_frac: sample fraction from the raw dataframe\n",
    "    Output:\n",
    "        - df_checkins: Checkin dataframe where all users have at least 5 checkins and 1 home location\n",
    "    '''\n",
    "    \n",
    "    # Read data from the file and drop unnecessary columns\n",
    "    df_tmp = pd.read_csv(path).sample(frac=sample_frac).drop(columns=['Venue_ID','day'])\n",
    "    \n",
    "    # Latitude and Longitude correction\n",
    "    df_tmp.lat = df_tmp.lat.apply(correct_latitude)\n",
    "    df_tmp.lon = df_tmp.lon.apply(correct_longitude)\n",
    "    \n",
    "    # Construct df_homes and select only relevant homes\n",
    "    df_homes = df_tmp.loc[df_tmp.place.str.lower().str.contains('home' and 'private')].copy()\n",
    "    df_homes = select_relevant_homes(df_homes)\n",
    "    \n",
    "    # Select users with relevant homes from the raw data\n",
    "    df_tmp = df_tmp.loc[df_tmp['User_ID'].isin(df_homes.index)].copy()\n",
    "    \n",
    "    # Count the number of checkins for each user\n",
    "    df_tmp_grouped = df_tmp.groupby('User_ID').agg({'User_ID':'count'})\n",
    "    \n",
    "    # Define a set containing users with at least 5 checkins\n",
    "    users = set(df_tmp_grouped[df_tmp_grouped['User_ID']>5].index)\n",
    "    \n",
    "    # Construct df_checkins\n",
    "    df_checkins = df_tmp.loc[df_tmp['User_ID'].isin(users)].copy()\n",
    "    \n",
    "    # Convert 'local time' attribute to a pandas datetime\n",
    "    df_checkins['local_time'] = pd.to_datetime(df_checkins['local_time'])\n",
    "    \n",
    "    # Label Homes\n",
    "    df_checkins['Is_home'] = df_checkins.place.str.lower().str.contains('home' and 'private')\n",
    "    \n",
    "    # Drop unnecessary column\n",
    "    df_checkins.drop(columns = ['place'],inplace = True)\n",
    "    \n",
    "    return df_checkins.sort_values(by=['User_ID','local_time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prediction_dataset(path, sample_frac = 1):\n",
    "    '''\n",
    "    This function takes the path of the raw data, import it and construct a checkin dataframe where\n",
    "    all users have at least 5 checkins. It's used to prepare the dataset we will use to predict home location\n",
    "    of the users.\n",
    "    \n",
    "    Input:\n",
    "        - Path: the Path of the file containing the data\n",
    "        - sample_frac: sample fraction from the raw dataframe\n",
    "    Output:\n",
    "        - df_checkins: Checkin dataframe where all users have at least 5 checkins\n",
    "    '''\n",
    "    \n",
    "    # Import dataset\n",
    "    df_checkins = pd.read_csv(path,sep='\\t',header=None,names=['User_ID','local_time','lat','lon','location_id'],\n",
    "                              parse_dates = ['local_time'])\n",
    "    # Drop NaN values\n",
    "    df_checkins.dropna(inplace = True)\n",
    "    \n",
    "    # Drop unnecessary column\n",
    "    df_checkins.drop(columns = ['location_id'],inplace=True)\n",
    "    \n",
    "    # Correct latitude and longitude\n",
    "    df_checkins.lat = df_checkins.lat.apply(correct_latitude)\n",
    "    df_checkins.lon = df_checkins.lon.apply(correct_longitude)\n",
    "    \n",
    "    # Drop checkins with both latitude and longitude set at zero\n",
    "    # Note: this is specific to gowalla and brightkite dataset\n",
    "    df_checkins = df_checkins[(df_checkins['lat'] !=0) & (df_checkins['lon'] != 0)]\n",
    "    \n",
    "    # Grouping by users\n",
    "    users = df_checkins.groupby(['User_ID']).agg({'User_ID':'count'})\n",
    "    \n",
    "    # Selecting users with more than 5 checkins\n",
    "    users = set(users.loc[users['User_ID']>5].index)\n",
    "    \n",
    "    df_checkins = df_checkins.loc[df_checkins['User_ID'].isin(users)]\n",
    "    \n",
    "    return df_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clusters_labels(df_user,clustering_method):\n",
    "    '''\n",
    "    This function clusters the checkins for a single user.\n",
    "    \n",
    "    Input:\n",
    "        - df_user: a dataframe containing the latitude and longitude for each checkin\n",
    "        - clustering_method: DBSCAN, we define this parameter to avoid unnecessary initialisations\n",
    "        when calling this funcrion\n",
    "    Output:\n",
    "        - clusters_labels: cluster label assigned to each checkin\n",
    "    '''\n",
    "    cluster_lables = clustering_method.fit(np.deg2rad(df_user[['lat','lon']])).labels_\n",
    "    \n",
    "    return cluster_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_user(df_user):\n",
    "    '''\n",
    "    To avoid biasing the dataset with multiple checkins in a small period of time or small distance traveled, \n",
    "    we drop checkins that are consecutively shared within 60 minutes and 100m.\n",
    "    \n",
    "    Input:\n",
    "        - df_user: datafame containing checkin time and location sorted by time\n",
    "    Output:\n",
    "        - df_user: cleaned df_user\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Constructing a dataframe containing the actual checkin and the next checkin\n",
    "    df_tmp = df_user.reset_index().merge(df_user.iloc[1:].reset_index(drop=True),right_index=True,\n",
    "                                         left_index=True,how='inner')\n",
    "    \n",
    "    # Compute the time between two consecutive checkins\n",
    "    df_tmp['dt'] = df_tmp['local_time_y'] - df_tmp['local_time_x']\n",
    "    \n",
    "    # Compute the distance between two consecutive checkins\n",
    "    columns = ['lat_x','lon_x','lat_y','lon_y']\n",
    "    df_tmp['distance'] = compute_distance(df_tmp,columns)\n",
    "    \n",
    "    # Construct a mask to keep consecutive checkins if they are distant by 60 minutes or 100m\n",
    "    # We also ignore checkins with 0 dt\n",
    "    mask = (df_tmp['dt']!=timedelta(0))&((df_tmp['dt']>timedelta(hours=1))|(df_tmp['distance']>0.1))\n",
    "    \n",
    "    return df_user.reset_index().iloc[df_tmp[mask].index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_checkin_during_midnight(df_user):\n",
    "    '''\n",
    "    This function lables the checkins after midnight.\n",
    "    \n",
    "    Input:\n",
    "        - df_user: dataframe containing and sorted by checkin time\n",
    "    Output:\n",
    "        - Labeles for each checkin. If it is happening after midnight and befor 7am it's set to True\n",
    "        and False otherwise\n",
    "    '''\n",
    "    df_tmp = (df_user['local_time'].dt.hour>=0) & (df_user['local_time'].dt.hour<7)\n",
    "    \n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_last_checkin(df_user):\n",
    "    '''\n",
    "    This function labels the last checkin before 3 am.\n",
    "    \n",
    "    Input:\n",
    "        df_user: Dataframe containing and sorted by checkin time\n",
    "    Output:\n",
    "        - Labeles for each checkin. If it is the last checkin of the day, the label is set to True\n",
    "        and False otherwise\n",
    "    '''\n",
    "    # We subsctract 3 hours  so we can detect the last checkin whenever the date changes\n",
    "    tmp_date = (df_user['local_time']-timedelta(hours=3)).dt.date.values\n",
    "    tmp_hour = (df_user['local_time']-timedelta(hours=3)).dt.hour.values\n",
    "    last_checkin = []\n",
    "    \n",
    "    # Labeling last checkins\n",
    "    for i in range(len(tmp_date)-1):\n",
    "        if (tmp_hour[i]>=14) and (tmp_hour[i]<=23) and (tmp_date[i]<tmp_date[i+1]):\n",
    "            last_checkin.append(True)\n",
    "        else:\n",
    "            last_checkin.append(False)\n",
    "            \n",
    "    # The last checkin is always True by definition \n",
    "    if (tmp_hour[-1]>=14) and (tmp_hour[-1]<=23):\n",
    "        last_checkin.append(True)\n",
    "    else:\n",
    "        last_checkin.append(False)\n",
    "\n",
    "    return last_checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_last_checkin_with_inactive_midnight(df_user):\n",
    "    '''\n",
    "    This function labels the last checkin with inactive midnight (no checkins between 0am and 7am).\n",
    "    \n",
    "    Input:\n",
    "        df_user: Dataframe containing and sorted by checkin time\n",
    "    Output:\n",
    "        - Labeles for each checkin. If it is the last checkin of the day and the user didn't checkin\n",
    "        between 0am and 7am the label is True and False otherwise\n",
    "    '''\n",
    "    \n",
    "    # Substract 7 hours to detect the change of the day whenever the date changes\n",
    "    tmp_date = (df_user['local_time']-timedelta(hours=7)).dt.date.values\n",
    "    tmp_hour = (df_user['local_time']-timedelta(hours=3)).dt.hour.values\n",
    "    \n",
    "    last_checkin_with_inactive_midnight = []\n",
    "    \n",
    "    # Compute the last checkin with inactive midnight\n",
    "    for i in range(len(tmp_date)-1):\n",
    "        # If the date changes and the hour is <= 23 the last checkin is happening before midnight\n",
    "        if (tmp_hour[i]>=14) and (tmp_hour[i]<=23) and (tmp_date[i]<tmp_date[i+1]):\n",
    "            last_checkin_with_inactive_midnight.append(True)\n",
    "        else:\n",
    "            last_checkin_with_inactive_midnight.append(False)\n",
    "    \n",
    "        # As the last checkin is by definition the last checkin of the day, we simply need to see if it is \n",
    "        # happening before midnight\n",
    "    \n",
    "    if (tmp_hour[-1]>=14) and (tmp_hour[-1]<=23):\n",
    "        last_checkin_with_inactive_midnight.append(True)\n",
    "    else:\n",
    "        last_checkin_with_inactive_midnight.append(False)\n",
    "        \n",
    "    return last_checkin_with_inactive_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dt_to_next_checkin(df_user):\n",
    "    '''\n",
    "    This function compute delta time between two consecutive checkins in Hour.\n",
    "    \n",
    "    Input:\n",
    "        - df_user: dataframe containing checkin time\n",
    "    Output:\n",
    "        - Delta time between two consecutive checkins\n",
    "    '''\n",
    "    \n",
    "    # get checkin times\n",
    "    checkin_time = df_user['local_time'].values\n",
    "    \n",
    "    # Compute the difference (The result is in nanoseconds)\n",
    "    delta_time = checkin_time[1:]-checkin_time[:-1]\n",
    "    \n",
    "    # Convert delta_time to hours\n",
    "    delta_time = delta_time.astype(float)/(1e9*3600)\n",
    "    \n",
    "    # The last checkin doesn't have a next checkin so we append None\n",
    "    delta_time = np.append(delta_time,None)\n",
    "    return delta_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_PR_RPR(df_user):\n",
    "    '''\n",
    "    This function computes the PageRank and ReversePageRank for each cluster. The ReversePageRank is computed\n",
    "    by inverting the the edges. The weight of an edge is obtained by computing the sum over clusters of the \n",
    "    inverse of the time made between two consecutive checkins\n",
    "    \n",
    "    Input:\n",
    "        - df_user: dataframe containing clusters labels for each checkin and the time until next checkin\n",
    "    Output:\n",
    "        - PageRank and ReversePageRank\n",
    "    '''\n",
    "    # Construct a dataframe containing the acutal checkin and next checkin\n",
    "    df_tmp = df_user.reset_index().iloc[:-1].merge(df_user.iloc[1:].reset_index(),\n",
    "                                                    right_index=True,left_index=True)\n",
    "    \n",
    "    # Compute the inverse time made between two consecutive checkins\n",
    "    df_tmp['inverse_time'] = 1/df_tmp['dt_to_next_checkin_x']\n",
    "    \n",
    "    # Construct the graph edges dataframe\n",
    "    df_graph = df_tmp.groupby(['cluster_label_x','cluster_label_y'],as_index = False).agg({'inverse_time':'sum'})\n",
    "    \n",
    "    # Initialise PageRank Graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Initialise ReversePageRank Graph\n",
    "    RG = nx.DiGraph()\n",
    "    \n",
    "    # Building edges of the two graphs\n",
    "    for i, row in df_graph.iterrows():\n",
    "        G.add_edge(int(row['cluster_label_x']),int(row['cluster_label_y']),weight=row['inverse_time'])\n",
    "        RG.add_edge(int(row['cluster_label_y']),int(row['cluster_label_x']),weight=row['inverse_time'])\n",
    "    \n",
    "    PR = list(nx.pagerank(G, max_iter = 10000, weight='weight').values())\n",
    "    RPR = list(nx.pagerank(RG, max_iter = 10000, weight='weight').values())\n",
    "    return PR, RPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_country(column):\n",
    "    '''\n",
    "    This function compute the most visited country in a cluster and assign it as label to the cluster.\n",
    "    \n",
    "    Input: \n",
    "        - column: column containing checkin countries\n",
    "    Output:\n",
    "        - most visited country\n",
    "    '''\n",
    "    \n",
    "    values,counts = np.unique(column.astype(str),return_counts=True)\n",
    "    \n",
    "    return values[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(path, training = True, sample_frac = 1):\n",
    "    '''\n",
    "    This is the main function to extract features from checkin data.\n",
    "    \n",
    "    Input:\n",
    "        - path: the path of the file containing raw checkins.\n",
    "        - training: if this parameter is True, construct training features and testing features if it is False.\n",
    "        - sample_frac: sample fraction from the raw dataframe\n",
    "    Output:\n",
    "        - Cleaned training data: dataframe containing features for each cluster of every user\n",
    "    '''\n",
    "    \n",
    "    if training:\n",
    "        # Initialize output dataframe for training dataset\n",
    "        df_tmp = pd.DataFrame(columns = ['user','CR','MR','EDR','EIDR','PR','RPR','Is_home',\n",
    "                                         'lat','lon','country'])\n",
    "        # Construct checkin dataframe\n",
    "        df_checkins  = construct_df_checkins(path, sample_frac=sample_frac)\n",
    "        \n",
    "    else:\n",
    "        # Initialize output dataframe for dataset to predict\n",
    "        df_tmp = pd.DataFrame(columns = ['user','CR','MR','EDR','EIDR','PR','RPR',\n",
    "                                         'lat','lon'])\n",
    "        # Copy dataset to predict\n",
    "        df_checkins = construct_prediction_dataset(path, sample_frac=sample_frac)\n",
    "    \n",
    "    # Exctract users from the checkin dataframe\n",
    "    users_id = np.unique(df_checkins['User_ID'])\n",
    "    \n",
    "    # Grouping the checkin dataframe by the 'User ID'\n",
    "    grouped_checkins = df_checkins.groupby('User_ID')\n",
    "    \n",
    "    # Initialize Clustering method with the right parameters\n",
    "    KMS_PER_RADIAN = 6371.0088\n",
    "    PRECISION = 0.1\n",
    "    clustering_method = DBSCAN(eps=PRECISION/KMS_PER_RADIAN,metric='haversine')\n",
    "    \n",
    "    # Compute features for each cluster of every user\n",
    "    for user in users_id:\n",
    "        \n",
    "        # Get the user Dataframe\n",
    "        df_user = grouped_checkins.get_group(user)\n",
    "        \n",
    "        # Clean df_user\n",
    "        df_user = cleaning_user(df_user).copy()\n",
    "        \n",
    "        # Consider only dataframes containing more than 1 cleaned entries\n",
    "        if len(df_user)>1:\n",
    "            \n",
    "            # Compute cluster_label\n",
    "            df_user['cluster_label'] = build_clusters_labels(df_user,clustering_method)\n",
    "            \n",
    "            # Compute Checkin during midnight\n",
    "            df_user['checkin_during_midnight'] = compute_checkin_during_midnight(df_user)\n",
    "            \n",
    "            # Compute last checkin\n",
    "            df_user['last_checkin'] = compute_last_checkin(df_user)\n",
    "            \n",
    "            # Compute last checkin with inactive midnight\n",
    "            df_user['last_checkin_with_inactive_midnight'] = compute_last_checkin_with_inactive_midnight(df_user)\n",
    "            \n",
    "            # Compute distance to next_checkin and classify edges\n",
    "            df_user['dt_to_next_checkin'] = compute_dt_to_next_checkin(df_user)\n",
    "            \n",
    "            #print(df_user)\n",
    "            # Construct aggregation dictionnary\n",
    "            if training:\n",
    "                agg_dic = {'cluster_label':'count','checkin_during_midnight':'sum',\n",
    "                            'last_checkin':'sum','last_checkin_with_inactive_midnight': 'sum',\n",
    "                            'Is_home': 'sum','lat':'mean','lon':'mean','country':compute_country}\n",
    "            else:\n",
    "                agg_dic = {'cluster_label':'count','checkin_during_midnight':'sum',\n",
    "                            'last_checkin':'sum','last_checkin_with_inactive_midnight': 'sum',\n",
    "                            'lat':'mean','lon':'mean'}\n",
    "                \n",
    "            # Construct rename dictiaonnary\n",
    "            rename_dic = {'cluster_label':'CR','checkin_during_midnight':'MR','last_checkin':'EDR',\n",
    "                          'last_checkin_with_inactive_midnight':'EIDR'}\n",
    "            \n",
    "            # Group by cluster_label\n",
    "            grouped_clusters = df_user.groupby('cluster_label')\n",
    "            \n",
    "            # Compute the first 4 features\n",
    "            features = grouped_clusters.agg(agg_dic).rename(columns = rename_dic)\n",
    "            \n",
    "            # Add user ID to the features\n",
    "            features['user'] = user\n",
    "            \n",
    "            # Compute Checkin Ration (CR)\n",
    "            features['CR'] = features['CR']/features['CR'].sum()\n",
    "            \n",
    "            # Compute Checkin Ration (MR)\n",
    "            features['MR'] = features['MR']/features['MR'].sum()\n",
    "            \n",
    "            # Compute Checkin Ration (EDR)\n",
    "            features['EDR'] = features['EDR']/features['EDR'].sum()\n",
    "            \n",
    "            # Compute Checkin Ration (EIDR)\n",
    "            features['EIDR'] = features['EIDR']/features['EIDR'].sum()\n",
    "            \n",
    "            if training:\n",
    "                # Label the clusters that contain the home location\n",
    "                features['Is_home'] = features['Is_home'] == features['Is_home'].max()\n",
    "            \n",
    "            # Compute PageRank and ReversePageRank\n",
    "            features['PR'],features['RPR'] = compute_PR_RPR(df_user)\n",
    "            \n",
    "            # Append results to the output dataframe\n",
    "            df_tmp = df_tmp.append(features)\n",
    "    \n",
    "    return df_tmp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = build_features(path = 'data/foursquare_checkin_data.csv.zip')\n",
    "df_training.dropna(inplace = True)\n",
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.to_csv('data/training_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_features = build_features(path = 'data/loc-gowalla_totalCheckins.txt.gz',training=False)\n",
    "df_gowalla_features.dropna(inplace = True)\n",
    "df_gowalla_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gowalla_features.to_csv('data/gowalla_checkin_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brightkite_features = build_features(path = 'data/loc-brightkite_totalCheckins.txt.gz',training=False)\n",
    "df_brightkite_features.dropna(inplace = True)\n",
    "df_brightkite_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brightkite_features.to_csv('data/brightkite_checkin_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
