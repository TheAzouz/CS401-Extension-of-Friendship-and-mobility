{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ayman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start by importing the data and exploring the column names and number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_checkin = pd.read_csv(\"data/processed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_checkin.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a ground truth dataframe that will contain the homes of users.\n",
    "\n",
    "We start by isolating the check-ins that have the `Home (private)` label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_checkins_homes = df_checkin[df_checkin.place == \"Home (private)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(df_checkins_homes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have 1724007 user check-ins into their homes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one issue with the dataset, whch is that the home location may vary certain users. Here, we sort the home check-ins by `User ID` and we see that for user 15, the home longitude and latitude may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_checkins_homes.sort_values(\"User ID\").head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe for user home locations, where we use the median location with label home for each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_homes = df_checkins_homes.groupby(\"User ID\")[[\"lat\", \"lon\"]].agg(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the check-in location labeled with `Home (private)` may not be unique for a certain user, we decide to keep only users whose home locations have a standard deviation of 100m around the mean location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `haversine` function computes the distance between two point on a sphere using thei longitudes and latitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "EARTH_RADIUS = 6371.0088\n",
    "def haversine(lat1, lat2, lon1, lon2):\n",
    "    sigma = np.arcsin(np.sqrt(np.sin((lat2-lat1)/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin((lon1-lon2)/2)**2))\n",
    "    return (2 * EARTH_RADIUS * sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select_relevant_homes` filters out the users having home locations with a standard deviation larger than 100m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_relevant_homes(df_homes):\n",
    "\n",
    "    # Grouping df_homes according to the user id and compute std and mean for lat and lon\n",
    "    df_homes = df_homes.groupby('User ID').agg({'lat':('std','mean'),'lon':('std','mean')})\n",
    "\n",
    "    # Filling nan values with 0 (std return 0 if there is only one sample)\n",
    "    df_homes.fillna(0,inplace = True)\n",
    "\n",
    "    # Compute distance from mean\n",
    "\n",
    "    # Preparing dataframe\n",
    "    df_tmp = pd.DataFrame()\n",
    "    df_tmp['lat1'] = df_homes.lat['mean']-df_homes.lat['std']\n",
    "    df_tmp['lat2'] = df_homes.lat['mean']+df_homes.lat['std']\n",
    "    df_tmp['lon1'] = df_homes.lon['mean']-df_homes.lon['std']\n",
    "    df_tmp['lon2'] = df_homes.lon['mean']+df_homes.lon['std']\n",
    "\n",
    "    # Compute distance\n",
    "    df_tmp['home_radius'] = haversine(df_tmp.lat1, df_tmp.lat2, df_tmp.lon1, df_tmp.lon2)\n",
    "\n",
    "    # Filter home and keep relevant home (estimated distance between homes checkins < 100m )\n",
    "    df_homes = df_homes[df_tmp['home_radius']<0.1][[('lat','mean'),('lon','mean')]].copy()\n",
    "\n",
    "    df_homes.columns = df_homes.columns.get_level_values(0)\n",
    "\n",
    "    return df_homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_homes_2 = select_relevant_homes(df_checkins_homes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_homes_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are left with 27784 users and their home locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home locations using discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us isolate the users that have homes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "users_with_homes = df_homes_2.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_checkin_filtered = df_checkin[df_checkin[\"User ID\"].isin(users_with_homes)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the code to discretize the world and detect home locations (from P2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "# Initialization\n",
    "pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to be called in lambda to get specific 25km*25km square where the coordinates are\n",
    "def square_coordinate(latitude, longitude):\n",
    "    lat_rad = np.deg2rad(latitude)\n",
    "    lon_rad = np.deg2rad(longitude)\n",
    "\n",
    "    square_side_on_equator = 10\n",
    "\n",
    "    # signed distance from equator\n",
    "    latitude_dist = EARTH_RADIUS * lat_rad\n",
    "\n",
    "    # signed distance from prime meridian (moving parallel to equator parallel to equator)\n",
    "    longitude_dist = EARTH_RADIUS * np.cos(lat_rad) * lon_rad\n",
    "\n",
    "    latitude_square_nbr = np.trunc(latitude_dist / square_side_on_equator)\n",
    "\n",
    "    # rescaling the length of the square side because of the curvature of the earth.\n",
    "    # on a cylindrical projection, horizontal distances are rescaled as follows\n",
    "    square_side_on_lat_line = square_side_on_equator * np.cos(lat_rad)\n",
    "\n",
    "    longitude_square_nbr = np.trunc(longitude_dist / square_side_on_lat_line)\n",
    "\n",
    "    return list(zip(latitude_square_nbr, longitude_square_nbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_home_coordinates(df):\n",
    "    df_ = df.copy()\n",
    "\n",
    "    # Get the square coordinates for each row\n",
    "    df_['square_coordinates'] = square_coordinate(df_.lat.values, df_.lon.values)\n",
    "\n",
    "    # Group by user then square, for each square create a list of the corresponding coordinates\n",
    "    df_ = df_.groupby(['User ID', 'square_coordinates'] ,as_index=False)\\\n",
    "                     [['User ID', 'square_coordinates', 'lat', 'lon']].agg(list)\n",
    "\n",
    "    # Compute number of checkins in each square per user\n",
    "    df_['freq'] = df_['lat'].str.len()\n",
    "\n",
    "    # Keep only the most frequented square for each user\n",
    "    df_ = df_.sort_values(['User ID','freq'], ascending=[True, False]).drop_duplicates(['User ID'])\n",
    "\n",
    "    # For each user, find the average location in the most frequented square\n",
    "    df_['lat'] = df_.lat.parallel_apply(lambda row: np.mean(row))\n",
    "    df_['lon'] = df_.lon.parallel_apply(lambda row: np.mean(row))\n",
    "\n",
    "    #return a dataframe with user, home latitude and home longitude\n",
    "    return df_[['User ID', 'lat', 'lon']].set_index('User ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now copute the home location of the users using the discretization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_detected_homes = get_home_coordinates(df_checkin_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_detected_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparizon between ground truth and discretization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute the distance between the home locations resulting from the discretization and the true home locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we merge the two dataframes, then we compute the distance between the true home and the detected home using the `haversine` function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_distance_detection_from_truth = pd.merge(df_homes_2, df_detected_homes, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_distance_detection_from_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_distance_detection_from_truth['dist'] = haversine(df_distance_detection_from_truth.lat_x,\n",
    "                                                     df_distance_detection_from_truth.lat_y,\n",
    "                                                     df_distance_detection_from_truth.lon_x,\n",
    "                                                     df_distance_detection_from_truth.lon_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_distance_detection_from_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we visualize the distribution of distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cumulative_dist_plot(series):\n",
    "    fig = plt.figure()\n",
    "    array = plt.hist(series, bins=1000, density=True, cumulative=True, bottom=0, histtype='step')\n",
    "    plt.close(fig)\n",
    "\n",
    "    x = array[1][1:]\n",
    "    y = array[0]\n",
    "\n",
    "    plt.plot(x, y, alpha=0.8, label='Distance from home CDF')\n",
    "\n",
    "    percentage = 0.85\n",
    "\n",
    "    i_percentage = next(i for i,v in enumerate(y) if v >= percentage)\n",
    "\n",
    "    y_percentage = y[i_percentage]\n",
    "    x_percentage = x[i_percentage]\n",
    "\n",
    "    plt.plot([10, x_percentage], [y_percentage, y_percentage], color='g', linestyle='dotted')\n",
    "    plt.plot([x_percentage, x_percentage], [0, y_percentage], color='g', linestyle='dotted')\n",
    "\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(10, 5*1e4)\n",
    "    plt.ylim(0, 1.1)\n",
    "\n",
    "    plt.yticks(sorted(list(plt.yticks()[0]) + [y_percentage])[:-1])\n",
    "\n",
    "    plt.annotate(f'{x_percentage:.0f} km', (x_percentage, 0), xytext=(x_percentage+1000, 0.12),\n",
    "                 arrowprops={'width':1, 'headlength':10, 'headwidth':5})\n",
    "\n",
    "    plt.ylabel(\"Cumulative density\", fontsize=12)\n",
    "    plt.xlabel(\"Distance from true home (km)\", fontsize=12)\n",
    "\n",
    "    plt.title(\"Cumulative distribution\\nof the distance between the detected\\nhome and the actual home\\n\",\n",
    "              fontsize=18)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cumulative_dist_plot(df_distance_detection_from_truth.dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that to attain a 85% accuracy, we need to consider detected home locations to be true up to a distance of 3'263 km from the true home location. This distance almost as much as the distance from Beirut to Valencia, i.e. larger than the Mediterranean Sea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore why this threshold distance is so large. We plot the distribution of distances on a log-log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def power_func(x, a, b):\n",
    "    \"\"\"\n",
    "    function to compute a*(x^b)\n",
    "    \"\"\"\n",
    "    return a * np.power(x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def density_loglog_dist_plot(series):\n",
    "    fig = plt.figure()\n",
    "    array = plt.hist(series, bins=1000, log=True, density=True, bottom=0, histtype='step')\n",
    "    plt.close(fig)\n",
    "\n",
    "    plt.loglog(array[1][1:], array[0], alpha=0.5, label='Distance from home probability density')\n",
    "\n",
    "    start = 4\n",
    "\n",
    "    distribution = array[0][start:]\n",
    "    distances = array[1][start+1:]\n",
    "\n",
    "    [a, b], _ = curve_fit(power_func, distances, distribution)\n",
    "\n",
    "    x = range(round(distances[0]), round(distances[-1]))\n",
    "    y = power_func(x, a, b)\n",
    "\n",
    "    plt.loglog(x, y, color='r', label=r'$%.4f*x^{%.4f}$' % (a,b))\n",
    "\n",
    "    plt.ylim(1e-6, 1e-2)\n",
    "    plt.xlim(10, 5*1e4)\n",
    "\n",
    "    plt.ylabel(\"Probability density\", fontsize=12)\n",
    "    plt.xlabel(\"Distance from true home (km)\", fontsize=12)\n",
    "\n",
    "    plt.title(\"Probability distribution\\nof the distance between the detected\\nhome and the actual home\\n\",\n",
    "              fontsize=18)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "density_loglog_dist_plot(df_distance_detection_from_truth.dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Percentiles and interpretation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Rami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from haversine import haversine_vector, Unit\n",
    "from sklearn.cluster import DBSCAN\n",
    "from datetime import timedelta\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def correct_latitude(lat):\n",
    "    \"\"\"\n",
    "    This function corrects for out of range latitude.\n",
    "\n",
    "    Input:\n",
    "    -- lat: latitude coordinates in °\n",
    "    Output:\n",
    "    -- lat: latitude coordinates put between -90 and 90°\n",
    "    \"\"\"\n",
    "    while lat>90 or lat<-90:\n",
    "        if lat>90:\n",
    "            lat = -(lat-180)\n",
    "        elif lat<-90:\n",
    "            lat = -(lat+180)\n",
    "    return lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def correct_longitude(long):\n",
    "    \"\"\"\n",
    "    This function corrects for out of range longitude.\n",
    "\n",
    "    Input:\n",
    "    -- long: longitude coordiantes in °\n",
    "    Output:\n",
    "    -- long: longitude coordinates put between -180 and 180°\n",
    "    \"\"\"\n",
    "    while long>180 or long<-180:\n",
    "        if long>180:\n",
    "            long = long - 360\n",
    "        elif long<-180:\n",
    "            long = long +360\n",
    "    return long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_distance(df,columns):\n",
    "    '''\n",
    "    This function computes the distance between two geographic coordinates for a given dataframe.\n",
    "\n",
    "    Input:\n",
    "        - df: Dataframe containing 4 columns latitude1, longitude1, latitude2 and longitude2\n",
    "        - columns: list of columns [latitude1, longitude1, latitude2 and longitude2]\n",
    "\n",
    "    Output:\n",
    "        - numpy array containing the distance between geographic coordinates of each row\n",
    "    '''\n",
    "    points1 = list(zip(df[columns[0]],df[columns[1]]))\n",
    "    points2 = list(zip(df[columns[2]],df[columns[3]]))\n",
    "    # Use harvesine_vector to compute the distance between points\n",
    "    return np.round(haversine_vector(points1,points2,Unit.KILOMETERS),decimals=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_relevant_homes(df_homes):\n",
    "    '''\n",
    "    This function selects relevant homes. We consider a home location as relevant if it's latitude and\n",
    "    longitude doesn't \"vary much\". To measure this variation, we simply compute the mean and the std of\n",
    "    the latitude and longitude of homes for every user. Then we construct 4 points as follow:\n",
    "        - by adding and substracting the standard deviation of the latitude and longitude from their\n",
    "        respective mean\n",
    "        - Measure the diagonal in KM\n",
    "        - If the diagonal is less than 100m we can assume with confidence that the mean is indeed the\n",
    "        home location\n",
    "\n",
    "    Input:\n",
    "        - df_homes: A dataframe containing all checkins labled as Home\n",
    "    Output:\n",
    "        - df_homes: Home location for each user\n",
    "    '''\n",
    "\n",
    "    # Grouping df_homes according to the user id and compute std and mean for lat and lon\n",
    "    df_homes = df_homes.groupby('User ID').agg({'lat':('std','mean'),'lon':('std','mean')})\n",
    "\n",
    "    # Filling nan values with 0 (std return 0 if there is only one sample)\n",
    "    df_homes.fillna(0,inplace = True)\n",
    "\n",
    "    # Construct the diagonal points\n",
    "    df_tmp = pd.DataFrame()\n",
    "    df_tmp['lat1'] = df_homes.lat['mean']-df_homes.lat['std']\n",
    "    df_tmp['lat2'] = df_homes.lat['mean']+df_homes.lat['std']\n",
    "    df_tmp['lon1'] = df_homes.lon['mean']-df_homes.lon['std']\n",
    "    df_tmp['lon2'] = df_homes.lon['mean']+df_homes.lon['std']\n",
    "\n",
    "    # Compute diagonal length\n",
    "    df_tmp['home_radius'] = compute_distance(df_tmp,['lat1','lon1','lat2','lon2'])\n",
    "\n",
    "    # Filter home and keep relevant home (estimated distance between homes checkins < 100m )\n",
    "    df_homes = df_homes[df_tmp['home_radius']<0.1][[('lat','mean'),('lon','mean')]].copy()\n",
    "\n",
    "    # Flatten df_homes columns\n",
    "    df_homes.columns = df_homes.columns.get_level_values(0)\n",
    "\n",
    "    return df_homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def construct_df_checkins(path,sample_frac = 1):\n",
    "    '''\n",
    "    This function takes the path of the raw data, import it and construct a checkin dataframe where\n",
    "    all users have at least 5 checkins and 1 home location\n",
    "\n",
    "    Input:\n",
    "        - Path: the Path of the file containing the data\n",
    "        - sample_frac: sample fraction from the raw dataframe\n",
    "    Output:\n",
    "        - df_checkins: Checkin dataframe where all users have at least 5 checkins and 1 home location\n",
    "    '''\n",
    "\n",
    "    # Read data from the file and drop unnecessary columns\n",
    "    df_tmp = pd.read_csv(path).sample(frac=sample_frac).drop(columns=['Venue ID','day'])\n",
    "\n",
    "    # Latitude and Longitude correction\n",
    "    df_tmp.lat = df_tmp.lat.apply(correct_latitude)\n",
    "    df_tmp.lon = df_tmp.lon.apply(correct_longitude)\n",
    "\n",
    "    # Construct df_homes and select only relevant homes\n",
    "    df_homes = df_tmp.loc[df_tmp.place.str.lower().str.contains('home' and 'private')].copy()\n",
    "    df_homes = select_relevant_homes(df_homes)\n",
    "\n",
    "    # Select users with relevant homes from the raw data\n",
    "    df_tmp = df_tmp.loc[df_tmp['User ID'].isin(df_homes.index)].copy()\n",
    "\n",
    "    # Count the number of checkins for each user\n",
    "    df_tmp_grouped = df_tmp.groupby('User ID').agg({'User ID':'count'})\n",
    "\n",
    "    # Define a set containing users with at least 5 checkins\n",
    "    users = set(df_tmp_grouped[df_tmp_grouped['User ID']>5].index)\n",
    "\n",
    "    # Construct df_checkins\n",
    "    df_checkins = df_tmp.loc[df_tmp['User ID'].isin(users)].copy()\n",
    "\n",
    "    # Convert 'local time' attribute to a pandas datetime\n",
    "    df_checkins['local time'] = pd.to_datetime(df_checkins['local time'])\n",
    "\n",
    "    # Label Homes\n",
    "    df_checkins['Is_home'] = df_checkins.place.str.lower().str.contains('home' and 'private')\n",
    "\n",
    "    # Drop unnecessary column\n",
    "    df_checkins.drop(columns = ['place'],inplace = True)\n",
    "\n",
    "    return df_checkins.sort_values(by=['User ID','local time']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_clusters_labels(df_user,clustering_method):\n",
    "    '''\n",
    "    This function clusters the checkins for a single user.\n",
    "\n",
    "    Input:\n",
    "        - df_user: a dataframe containing the latitude and longitude for each checkin\n",
    "        - clustering_method: DBSCAN, we define this parameter to avoid unnecessary initialisations\n",
    "        when calling this funcrion\n",
    "    Output:\n",
    "        - clusters_labels: cluster label assigned to each checkin\n",
    "    '''\n",
    "    cluster_lables = clustering_method.fit(np.deg2rad(df_user[['lat','lon']])).labels_\n",
    "\n",
    "    return cluster_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning_user(df_user):\n",
    "    '''\n",
    "    To avoid biasing the dataset with multiple checkins in a small period of time or small distance traveled,\n",
    "    we drop checkins that are consecutively shared within 60 minutes and 100m.\n",
    "\n",
    "    Input:\n",
    "        - df_user: datafame containing checkin time and location sorted by time\n",
    "    Output:\n",
    "        - df_user: cleaned df_user\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Constructing a dataframe containing the actual checkin and the next checkin\n",
    "    df_tmp = df_user.reset_index().merge(df_user.iloc[1:].reset_index(drop=True),right_index=True,\n",
    "                                         left_index=True,how='inner')\n",
    "\n",
    "    # Compute the time between two consecutive checkins\n",
    "    df_tmp['dt'] = df_tmp['local time_y'] - df_tmp['local time_x']\n",
    "\n",
    "    # Compute the distance between two consecutive checkins\n",
    "    columns = ['lat_x','lon_x','lat_y','lon_y']\n",
    "    df_tmp['distance'] = compute_distance(df_tmp,columns)\n",
    "\n",
    "    # Construct a mask to keep consecutive checkins if they are distant by 60 minutes or 100m\n",
    "    # We also ignore checkins with 0 dt\n",
    "    mask = (df_tmp['dt']!=timedelta(0))&((df_tmp['dt']>timedelta(hours=1))|(df_tmp['distance']>0.1))\n",
    "\n",
    "    return df_user.reset_index().iloc[df_tmp[mask].index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_checkin_during_midnight(df_user):\n",
    "    '''\n",
    "    This function lables the checkins after midnight.\n",
    "\n",
    "    Input:\n",
    "        - df_user: dataframe containing and sorted by checkin time\n",
    "    Output:\n",
    "        - Labeles for each checkin. If it is happening after midnight and befor 7am it's set to True\n",
    "        and False otherwise\n",
    "    '''\n",
    "    df_tmp = (df_user['local time'].dt.hour>=0) & (df_user['local time'].dt.hour<7)\n",
    "\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_last_checkin(df_user):\n",
    "    '''\n",
    "    This function labels the last checkin before 3 am.\n",
    "\n",
    "    Input:\n",
    "        df_user: Dataframe containing and sorted by checkin time\n",
    "    Output:\n",
    "        - Labeles for each checkin. If it is the last checkin of the day, the label is set to True\n",
    "        and False otherwise\n",
    "    '''\n",
    "    # We subsctract 3 hours  so we can detect the last checkin whenever the date changes\n",
    "    tmp_date = (df_user['local time']-timedelta(hours=3)).dt.date.values\n",
    "    last_checkin = []\n",
    "\n",
    "    # Labeling last checkins\n",
    "    for i in range(len(tmp_date)-1):\n",
    "        if tmp_date[i]<tmp_date[i+1]:\n",
    "            last_checkin.append(True)\n",
    "        else:\n",
    "            last_checkin.append(False)\n",
    "\n",
    "    # The last checkin is always True by definition\n",
    "    last_checkin.append(True)\n",
    "\n",
    "    return last_checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_last_checkin_with_inactive_midnight(df_user):\n",
    "    '''\n",
    "    This function labels the last checkin with inactive midnight (no checkins between 0am and 7am).\n",
    "\n",
    "    Input:\n",
    "        df_user: Dataframe containing and sorted by checkin time\n",
    "    Output:\n",
    "        - Labeles for each checkin. If it is the last checkin of the day and the user didn't checkin\n",
    "        between 0am and 7am the label is True and False otherwise\n",
    "    '''\n",
    "\n",
    "    # Substract 7 hours to detect the change of the day whenever the date changes\n",
    "    tmp_date = (df_user['local time']-timedelta(hours=7)).dt.date.values\n",
    "    tmp_hour = (df_user['local time']).dt.hour.values\n",
    "\n",
    "    last_checkin_with_inactive_midnight = []\n",
    "\n",
    "    # Compute the last checkin with inactive midnight\n",
    "    for i in range(len(tmp_date)-1):\n",
    "        # If the date changes and the hour is <= 23 the last checkin is happening before midnight\n",
    "        if (tmp_date[i]<tmp_date[i+1]) and (tmp_hour[i]<=23):\n",
    "            last_checkin_with_inactive_midnight.append(True)\n",
    "        else:\n",
    "            last_checkin_with_inactive_midnight.append(False)\n",
    "    # As the last checkin is by definition the last checkin of the day, we simply need to see if it is\n",
    "    # happening before midnight\n",
    "    if tmp_hour[-1]<=23:\n",
    "        last_checkin_with_inactive_midnight.append(True)\n",
    "    else:\n",
    "        last_checkin_with_inactive_midnight.append(False)\n",
    "\n",
    "    return last_checkin_with_inactive_midnight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_dt_to_next_checkin(df_user):\n",
    "    '''\n",
    "    This function compute delta time between two consecutive checkins in Hour.\n",
    "\n",
    "    Input:\n",
    "        - df_user: dataframe containing checkin time\n",
    "    Output:\n",
    "        - Delta time between two consecutive checkins\n",
    "    '''\n",
    "\n",
    "    # get checkin times\n",
    "    checkin_time = df_user['local time'].values\n",
    "\n",
    "    # Compute the difference (The result is in nanoseconds)\n",
    "    delta_time = checkin_time[1:]-checkin_time[:-1]\n",
    "\n",
    "    # Convert delta_time to hours\n",
    "    delta_time = delta_time.astype(float)/(1e9*3600)\n",
    "\n",
    "    # The last checkin doesn't have a next checkin so we append None\n",
    "    delta_time = np.append(delta_time,None)\n",
    "    return delta_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_PR_RPR(df_user):\n",
    "    '''\n",
    "    This function computes the PageRank and ReversePageRank for each cluster. The ReversePageRank is computed\n",
    "    by inverting the the edges. The weight of an edge is obtained by computing the sum over clusters of the\n",
    "    inverse of the time made between two consecutive checkins\n",
    "\n",
    "    Input:\n",
    "        - df_user: dataframe containing clusters labels for each checkin and the time until next checkin\n",
    "    Output:\n",
    "        - PageRank and ReversePageRank\n",
    "    '''\n",
    "    # Construct a dataframe containing the acutal checkin and next checkin\n",
    "    df_tmp = df_user.reset_index().iloc[:-1].merge(df_user.iloc[1:].reset_index(),\n",
    "                                                    right_index=True,left_index=True)\n",
    "\n",
    "    # Compute the inverse time made between two consecutive checkins\n",
    "    df_tmp['inverse_time'] = 1/df_tmp['dt_to_next_checkin_x']\n",
    "\n",
    "    # Construct the graph edges dataframe\n",
    "    df_graph = df_tmp.groupby(['cluster_label_x','cluster_label_y'],as_index = False).agg({'inverse_time':'sum'})\n",
    "\n",
    "    # Initialise PageRank Graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Initialise ReversePageRank Graph\n",
    "    RG = nx.DiGraph()\n",
    "\n",
    "    # Building edges of the two graphs\n",
    "    for i, row in df_graph.iterrows():\n",
    "        G.add_edge(int(row['cluster_label_x']),int(row['cluster_label_y']),weight=row['inverse_time'])\n",
    "        RG.add_edge(int(row['cluster_label_y']),int(row['cluster_label_x']),weight=row['inverse_time'])\n",
    "\n",
    "    return list(nx.pagerank(G, weight='weight').values()), list(nx.pagerank(RG, weight='weight').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ratio(column):\n",
    "    '''\n",
    "    This is an aggregation function to compute the ratio.\n",
    "\n",
    "    Input:\n",
    "        - column: column containing binary labels\n",
    "    Output:\n",
    "        - Ratio of positive labels\n",
    "    '''\n",
    "    return np.sum(column)/len(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_features(path, sample_frac = 1):\n",
    "    '''\n",
    "    This is the main function to extract features from checkin data.\n",
    "\n",
    "    Input:\n",
    "        - path: the path of the file containing raw checkins.\n",
    "        - sample_frac: sample fraction from the raw dataframe\n",
    "    Output:\n",
    "        - Cleaned training data: dataframe containing features for each cluster of every user\n",
    "    '''\n",
    "\n",
    "    # Construct checkin dataframe\n",
    "    df_checkins  = construct_df_checkins(path,sample_frac=sample_frac)\n",
    "\n",
    "    # Initialize output dataframe\n",
    "    df_tmp = pd.DataFrame(columns = ['user','cluster_id','CR','MR','EDR','EIDR','PR','RPR','Is_home'])\n",
    "\n",
    "    # Exctract users from the checkin dataframe\n",
    "    users_id = np.unique(df_checkins['User ID'])\n",
    "\n",
    "    # Grouping the checkin dataframe by the 'User ID'\n",
    "    grouped_checkins = df_checkins.groupby('User ID')\n",
    "\n",
    "    # Initialize Clustering method with the right parameters\n",
    "    KMS_PER_RADIAN = 6371.0088\n",
    "    PRECISION = 0.1\n",
    "    clustering_method = DBSCAN(eps=PRECISION/KMS_PER_RADIAN,metric='haversine')\n",
    "\n",
    "    # Compute features for each cluster of every user\n",
    "    for user in users_id:\n",
    "\n",
    "        # Get the user Dataframe\n",
    "        df_user = grouped_checkins.get_group(user)\n",
    "\n",
    "        # Clean df_user\n",
    "        df_user = cleaning_user(df_user).copy()\n",
    "\n",
    "        # Consider only dataframes containing more than 1 cleaned entries\n",
    "        if len(df_user)>1:\n",
    "\n",
    "            # Compute cluster_label\n",
    "            df_user['cluster_label'] = build_clusters_labels(df_user,clustering_method)\n",
    "\n",
    "            # Compute Checkin during midnight\n",
    "            df_user['checkin_during_midnight'] = compute_checkin_during_midnight(df_user)\n",
    "\n",
    "            # Compute last checkin\n",
    "            df_user['last_checkin'] = compute_last_checkin(df_user)\n",
    "\n",
    "            # Compute last checkin with inactive midnight\n",
    "            df_user['last_checkin_with_inactive_midnight'] = compute_last_checkin_with_inactive_midnight(df_user)\n",
    "\n",
    "            # Compute distance to next_checkin and classify edges\n",
    "            df_user['dt_to_next_checkin'] = compute_dt_to_next_checkin(df_user)\n",
    "\n",
    "            # Construct aggregation dictionnary\n",
    "            agg_dic = {'cluster_label':'count','checkin_during_midnight':compute_ratio,\n",
    "                        'last_checkin':compute_ratio,'last_checkin_with_inactive_midnight': compute_ratio,\n",
    "                        'Is_home': 'sum'}\n",
    "            # Construct rename dictiaonnary\n",
    "            rename_dic = {'cluster_label':'CR','checkin_during_midnight':'MR','last_checkin':'EDR',\n",
    "                          'last_checkin_with_inactive_midnight':'EIDR'}\n",
    "\n",
    "            # Group by cluster_label\n",
    "            grouped_clusters = df_user.groupby('cluster_label')\n",
    "\n",
    "            # Compute the first 4 features\n",
    "            features = grouped_clusters.agg(agg_dic).rename(columns = rename_dic)\n",
    "\n",
    "            # Add user ID to the features\n",
    "            features['user'] = user\n",
    "\n",
    "            # Compute Checkin Ration (CR)\n",
    "            features['CR'] = features['CR']/features['CR'].sum()\n",
    "\n",
    "            # Label the clusters that contain the home location\n",
    "            features['Is_home'] = features['Is_home'] == features['Is_home'].max()\n",
    "\n",
    "            # Compute PageRank and ReversePageRank\n",
    "            features['PR'],features['RPR'] = compute_PR_RPR(df_user)\n",
    "\n",
    "            # Append results to the output dataframe\n",
    "            df_tmp = df_tmp.append(features)\n",
    "\n",
    "    # Assign clusters_id to the dataframe\n",
    "    df_tmp['cluster_id'] = df_tmp.index\n",
    "\n",
    "    return df_tmp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_training = build_features('data/processed_dataset-003.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_training.to_csv('training_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_training.Is_home.sum()/len(df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Selim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/TheAzouz/Project_ADA/blob/main/selim_P4_group.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount ('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cd \"drive/My Drive/Colab Notebooks/ADA_P4/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_distance (lat1,lon1,lat2,lon2):\n",
    "    \"\"\"\n",
    "    ################## Function we used in P2 and P4 to calculte distances ####################\n",
    "\n",
    "    R : radius of earth : 6378.137 km\n",
    "    lat1,lon1 : latitude and longitude of one user\n",
    "    lat2,lon2 : latitude and longitude of other user\n",
    "    \"\"\"\n",
    "    R = 6378.137\n",
    "    # convert into radians\n",
    "    lat1_rad=np.deg2rad(lat1)\n",
    "    lat2_rad=np.deg2rad(lat2)\n",
    "    lon1_rad=np.deg2rad(lon1)\n",
    "    lon2_rad=np.deg2rad(lon2)\n",
    "\n",
    "    #get difference of lattitude and difference of longitude\n",
    "    delta_lat=lat2_rad-lat1_rad\n",
    "    delta_lon=lon2_rad-lon1_rad\n",
    "\n",
    "    #return formula Haversine formula\n",
    "    a=((np.sin(0.5*delta_lat))**2)+np.cos(lat1_rad)*np.cos(lat2_rad)*((np.sin(0.5*delta_lon))**2)\n",
    "    return 2*R*np.arcsin(np.sqrt(a))\n",
    "\n",
    "def cont (x):\n",
    "    if any(x.str.contains('Home','private')):return 1\n",
    "    else : return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2) Friends checkins:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to find checkin patterns between friends gaherings.\n",
    "- For that, we create a new dataframe, with whome we are are going to work for this whole part.\n",
    "- We assume two friends have met together if they have checked in the same place with at most one hour difference. Since our dataframe is labeled and each place has its own id, we don't need to do any approximation on the checkin distance between friends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def friends_gatherings(PROCEEDED_PATH,FRIENDSHIPS_PATH):\n",
    "    \"\"\"\n",
    "    function to get dataframe where friends gathered\n",
    "    INPUTS :\n",
    "    PROCEEDED_PATH : proceeded dataset path\n",
    "    FRIENDSHIPS_PATH : friendships (edges) path\n",
    "    OUTPUTS :\n",
    "    1) dataframe that will be used for 1 futur plot, contains two columns : distance from home and probability of checkin\n",
    "    This dataframe isn't limited to friends checkins and is used to see if there is any difference\n",
    "    between considering friends gatherings or not\n",
    "    2) Friends dataframe : contains the dataframe that will be used all this part for our studies.\n",
    "    This dataframe contains the columns ['User ID','Venue ID','day','local time','lat','lon','place','country']\n",
    "\n",
    "    In order to have a study that is the most accurate possible, we only work on users that checked in their homes at least once.\n",
    "    That helps us make no approximation on the home location.\n",
    "    Moreover, our dataframe being large, we don't have the problem of not having enough data to study.\n",
    "\n",
    "    In order to preserve memory, we will import the friendship dataframe in this function, we won't need it after that.\n",
    "    \"\"\"\n",
    "    #Import two datasets\n",
    "    df=pd.read_csv(PROCEEDED_PATH,parse_dates=['local time'])\n",
    "    friendships=pd.read_csv(FRIENDSHIPS_PATH,sep='\\t',encoding='latin-1',names=['User ID','User2 ID'], header=None)\n",
    "\n",
    "    #clean edges dataframe : erase columns where the person following and the person followed is the same\n",
    "    friendships=friendships[friendships['User ID']!=friendships['User2 ID']]\n",
    "\n",
    "    print('begin getting homes')\n",
    "    #create a new dataframe : df1 : used to find the home location of each user\n",
    "    #To do that,  we keep only the line where the user checked in their homes\n",
    "    #Then, we get the means of checkin latitude and longitude for each user and each place checked in to find the home location\n",
    "    # ==> The home location is the mean of checkins that were in the home of a user\n",
    "    # We finally return only the three columns needed to perform the final merge to have the final\n",
    "\n",
    "    #We just initialize the two columns we need\n",
    "    df1=df[['User ID','lat','lon','place']].rename(columns={'lat':'home lat','lon':'home lon'})\n",
    "    df1=df1.groupby(['User ID','place'],as_index=False).\\\n",
    "        agg({'home lat':'mean','home lon':'mean'},axis='columns')\n",
    "    df1=df1[df1['place'].str.contains('Home (private)',regex=False)][['User ID','home lat','home lon']]\n",
    "    #df1 now contains for each user his home longitude and home latitude\n",
    "\n",
    "    #We then perform a merge with the original dataset to integrate the home coordinates to the dataset\n",
    "    df1=df1.merge(df[['User ID','Venue ID','day','local time','lat','lon','place','country']])\n",
    "    df1['dist home']=calculate_distance(df1['lat'],df1['lon'],df1['home lat'],df1['home lon']).round()\n",
    "\n",
    "    #Now we move to finding the final dataframe (the one we will use in the future)\n",
    "    merged_friends=pd.DataFrame()\n",
    "    chunksize=10**6\n",
    "    numb_chunks=int(np.ceil(df1.shape[0]/chunksize))\n",
    "    print('begin merging')\n",
    "    #We work with chunks of size 10**6 each\n",
    "    for i in range(numb_chunks):\n",
    "        # Since we want the checkin place to be exactly the same for each user (have the same id),\n",
    "        #we perform the merge on both the user and his checkin place (this procedure helped us save much memory and time)\n",
    "        tmp_merge=df1[chunksize*i:chunksize*(i+1)].merge(friendships).\\\n",
    "                    merge(df1[['User ID','Venue ID','local time']],\\\n",
    "                          left_on=['User2 ID','Venue ID'],right_on=['User ID','Venue ID']).\\\n",
    "                    rename(columns={'dist home_x':'dist home'})\n",
    "\n",
    "        #filter the tmp_merge with friends that checked in at most with one hour difference in the same place\n",
    "        tmp_merge=tmp_merge[(np.abs((tmp_merge['local time_x']-tmp_merge['local time_y']).dt.total_seconds())<3600)]\n",
    "        tmp_merge=tmp_merge.rename(columns={'local time_x':'local time'})\n",
    "\n",
    "        #append the chunk to the final dataset\n",
    "        merged_friends=merged_friends.append(tmp_merge[['day','local time','place','country','dist home']],ignore_index=True)\n",
    "\n",
    "        if i==int(0.5*numb_chunks):print('halfway through merging')\n",
    "    print('finished merging')\n",
    "    return get_vects_plot(df1[['dist home']]),merged_friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_vects_plot(df):\n",
    "    \"\"\"\n",
    "    function to calculate the probability of checkin as a function of the distance\n",
    "    Returns a dataframe where there are two columns: the distance from home and its probiability\n",
    "\n",
    "    We begin by counting the number of checkin for each distance\n",
    "    The we divide by the total number of counts to get a  probability.\n",
    "    \"\"\"\n",
    "    #Create a new column, we will change it after\n",
    "    #This new column will contain the probability of probability of checking in knowing the distance from home\n",
    "    df1=df.copy()\n",
    "    df1['proba dist']=df1['dist home']\n",
    "\n",
    "    #'proba dist' column now contains the number of checkins for each distance\n",
    "    df1=df1.groupby('dist home',as_index=False).count()[['dist home','proba dist']]\n",
    "\n",
    "    #We divide the 'proba dist' column by the total number of checkins\n",
    "    df1.loc[:,'proba dist']=df1['proba dist']/df1['proba dist'].sum()\n",
    "    return df1[['dist home','proba dist']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROCEEDED_PATH='processed_dataset.csv'\n",
    "FRIENDSHIPS_PATH='dataset_WWW_friendship_new.txt'\n",
    "df_tot,df_friends=friends_gatherings(PROCEEDED_PATH,FRIENDSHIPS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def apply_median(y,N=6):\n",
    "    \"\"\"\n",
    "    ################  Function used also in P4 ################\n",
    "\n",
    "    smoth curve using median\n",
    "    y : to be plotted\n",
    "    N : Number of items to use to smooth curve\n",
    "    We choose not to do this process for small values because the curve is already smooth for small distances\n",
    "    \"\"\"\n",
    "    y1=np.copy(y)\n",
    "\n",
    "    for i in range (N,len(y)):\n",
    "        if i>N:y1[i]=np.median(y[0:i+N])\n",
    "        elif i<len(y)-N : y1[i]=np.median(y[i-N:i+N])\n",
    "        else : y1[i]=np.median(y[i-N:len(y)])\n",
    "\n",
    "    return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_friends.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def power_func(x, a, b):\n",
    "    \"\"\"\n",
    "    function to compute a*(x^b)\n",
    "    \"\"\"\n",
    "    return a * np.power( x,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_plot_friends=get_vects_plot(df_friends)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_plot_friends.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#The change_point isthe point where the behavior of the curves changes\n",
    "#We find the chnge point empirically\n",
    "change_point=16\n",
    "#We create a logspace x_axis, we will use this vector for our futur plots\n",
    "x_log=np.logspace(0,4.05,50)\n",
    "\n",
    "#For both dataframes, we do an interpolation to find the corresponding values in the new logspace axis\n",
    "#First, we begin by finding the interpolation functions for each vector,\n",
    "#Then, we apply them of the dataframes\n",
    "f_friends=interp1d(df_plot_friends['dist home'],df_plot_friends['proba dist'],kind='zero')\n",
    "f_tot=interp1d(df_tot['dist home'],df_tot['proba dist'],kind='zero')\n",
    "\n",
    "#y_friends,y_tot will be the vectors used for the plot.\n",
    "#We don't apply the  median because we want to see the true curve, not a smoothed one.\n",
    "y_friends=f_friends(x_log)\n",
    "y_tot=f_tot(x_log)\n",
    "\n",
    "#Since we want to approximate the parameters of our curves, we use scipy function 'fit_curve'\n",
    "#'params_friends1' are parameters for smaller distnces\n",
    "#'params_friends2' are parameters for bigger distances\n",
    "params_friends1,_=curve_fit(power_func,x_log[:change_point],y_friends[:change_point])\n",
    "params_friends2,_=curve_fit(power_func,x_log[change_point:],y_friends[change_point:])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "#Create marker style for both vectors and do the plots\n",
    "#Plot all checkins\n",
    "marker_style_all = dict(color='r', linestyle=':', marker=(6, 2, 0),markersize=16)\n",
    "plt.loglog(x_log,y_tot,markevery=0.05, **marker_style_all,label='all checkins')\n",
    "\n",
    "#Plot checkins proceeded only with friends\n",
    "marker_style_friends = dict(color='blue', linestyle=':', marker='o',markersize=16,fillstyle='none')\n",
    "plt.loglog(x_log,y_friends,markevery=0.05, **marker_style_friends,label='only meeting friends')\n",
    "plt.xlabel('distance from home')\n",
    "plt.ylabel('probability')\n",
    "plt.title('fraction of friends met as a function of the distance from home')\n",
    "\n",
    "#Now we proceed the plot of the approximation curve for smaller and larger distances\n",
    "plt.loglog(x_log[change_point-1:], power_func(x_log[change_point-1:], params_friends2[0], params_friends2[1]),\\\n",
    "                  color='g',linewidth=3,label='slopes (meeting friends)')\n",
    "plt.loglog(x_log[:change_point], power_func(x_log[:change_point], params_friends1[0], params_friends1[1]),\\\n",
    "                  color='g',linewidth=3)\n",
    "plt.axvline(x_log[change_point-1],linestyle='--',linewidth=1, color='grey')\n",
    "#plt.xticks([change_point+1],[round(x_log[change_point-1],0)])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('we have slopes of {} for distances inferior to 30km and slopes of {} for distances superior to 20km'.\\\n",
    "      format(round(params_friends1[1],2),round(params_friends2[1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('For distances inferior to 20km:\\nThe distribution checkins knowingthe distance from home can ba approximates as :\\\n",
    " P(x)={}*exp({}*x)'.format(round(params_friends1[0],2),round(params_friends1[1],2)))\n",
    "print('For distances superior to 20km:\\nthe distribution checkins knowingthe distance from home can ba approximates as :\\\n",
    " P(x)={}*exp({}*x)'.format(round(params_friends2[0],2),round(params_friends2[1],2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also approximate the probability of moving from home knowing the distance with two equations:\n",
    "\n",
    "$$ P(x)=\n",
    "\\begin{cases}\n",
    "    0.1e^{-0.44x} & \\text{if x<20 km}\\\\\n",
    "    0.35e^{-1.3x} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We notice a change in the slope at a distance of approximately 20km distance from home. This behavior is similar to the one described in the paper. However, some differences are noticeble:\n",
    "1) The shift happened in a distance of 20km from home (vs 100km using other daasets)\n",
    "2) The slope are different than the ones described in the paper.\n",
    "Firstly, the slopes generally are smaller in our study compared to the ones found in the paper.\n",
    "Secondly, while the slope is smaller for small distances in the paper (-1.9 < -0.9), the behaviour is different in our study (-0.44 > -1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, we notice in the plots that whether a user visited a friend or not does not make much differences in the overall behavior of checkins. We make the hypothesis that the behavior is the same and we test it. This hypothesis states that friends don't have any influence on a user's movement.\n",
    "- Before going into that, we test whether our data is normally distributed or not. We perform a Kolmogorov Smirnov test, a test made to check whether a distribution is normal or not. The  Null hypothesis is that the sample comes from a normal distribution. and we reject it depending on the pvalue we get. The p-value is the probability of obtaining results that aren't more extereme than the ones observed, if we assume the null hypothesis is true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats import diagnostic\n",
    "_, p_value_friends = diagnostic.kstest_normal(df_plot_friends['proba dist'], dist = 'norm')\n",
    "_, p_value_tot = diagnostic.kstest_normal(df_tot['proba dist'], dist = 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Having the Null hypothesis that the data is derived from a normal distribution, we get :\\n \\t pvalue {} \\\n",
    "for the total checkins \\n \\t pvalue {} for the checkins with only friends'.format(p_value_tot,p_value_friends))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The dataframe containing only checkins with friends contains {} ligns\\n\\\n",
    "The dataframe containing all checkins contains {} ligns'.format(df_plot_friends.shape[0],df_tot.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow$ Using the result found before, we conclude our data is not derived from a normal distribution\n",
    "\n",
    "- Now we test whether the behaviour is the same or not for the two sets (considering all checkins vs considering only checkins with friends). This will help us know whether friends do have an influence on people's movement or not.\n",
    "- We use the assertions below:\n",
    "1) We took two different samples from the same population.\n",
    "2) The data we're treating (probabilities) is not derived from a normal disrtibution (verified).\n",
    "3) Our samples are paired since they are derived from the same dataset\n",
    "$\\Rightarrow$  We do a Wilcoxon test.\n",
    "To test the null hypothesis that there is no difference in behavior, we can apply the two-sided test.\n",
    "\n",
    "- Our frames being of different shapes, we create a vector that will be used to make to comparision between the two frames and we use interpolations in order to have the the most precise values possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_log_exp=np.logspace(0,4.05,3000)\n",
    "\n",
    "#For both dataframes, we do an interpolation to find the corresponding values in the new logspace axis\n",
    "#First, we begin by finding the interpolation functions for each vector,\n",
    "#Then, we apply them of the dataframes\n",
    "f_friends_exp=interp1d(df_plot_friends['dist home'],df_plot_friends['proba dist'],kind='zero')\n",
    "f_tot_exp=interp1d(df_tot['dist home'],df_tot['proba dist'],kind='zero')\n",
    "\n",
    "#y_friends_exp,y_tot_exp will be the expanded (working with 3000 points) vectors used for the hypothesis testing\n",
    "y_friends_exp=f_friends_exp(x_log)\n",
    "y_tot_exp=f_tot_exp(x_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, p_value = wilcoxon(np.abs(y_friends_exp-y_tot_exp))\n",
    "print('The pvalue found after performing a wilcoxon test is :',np.round(p_value,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Having found a p-value of 0.0, we strongly reject our Null hypothesis.\n",
    "$\\Rightarrow$ We conclude that friends do have a significant importance on a user's mobility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3) Places checkin patterns**\n",
    "### I) Preprocessing :\n",
    "- From now on, we only work with checkins with friends only : Every thing we will be studying are meetings between friends.\n",
    "- We now go through our dataset and gategorize our places features into different categories:\n",
    "1) `Eat` : Going to eat with friends (restaurant , fast food ...)\n",
    "2) `Study` : Studying (being in school, universty ...)\n",
    "3) `Drink` : Having a drink with friends, going out ...\n",
    "4) `Culture` : Going to watch a movie, visit monuments ...\n",
    "5) `Home` : meet at someone's home\n",
    "6) `Move` : take public transports or travel to far places\n",
    "7) `Consume` : Visit stores, malls ...\n",
    "8) `Work` : Being in work's place\n",
    "9) `Entertain` : Go to a spa, hotel, beach,park ...\n",
    "10) `Sport` : practise sports together\n",
    "\n",
    "- We also categorize the days of the week into two types:\n",
    "1) `Working days` : Monday until Friday\n",
    "2) `Week end day` : Saturday and Sunday\n",
    "\n",
    "### II) Processing :\n",
    "- We study the probability people meet in each category\n",
    "- We compare normalized probabilities\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_classified=df_friends.copy()\n",
    "\n",
    "#Change the name of variables to its type (will be used after)\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('restaurant|Burger|pizza|Diner|food|Steakhouse|\\\n",
    "                  BBQ|Dessert|Ramen|Ice Cream|Fried|Sandwich|breakfast|snack|\\\n",
    "                  taco|hot|soup|wings',case=False),'place']='Eat'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('college|University|school|student',case=False),'place']='Study'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('coffee|Bar|Nightclub|pub|Lounge|Beer|tea|Nightlife',\\\n",
    "                           case=False),'place']='Drink'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('multiplex|Movie|Theater|concert|Music|historic|arts|\\\n",
    "                  Museum|library|Monument|temple|art',case=False),'place']='Culture'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('Home|Residential|Building',case=False),'place']='Home'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('station|airport|subway|travel|boat|bus',case=False),'place']='Move'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('store|mall|plaza|shop|boutique|market',case=False),'place']='Consume'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('work|office|Startup|professional',case=False),'place']='Work'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('soccer stadium|Entertainment|Outdoor|beach|park|event|\\\n",
    "                  Arcade|resort|hotel|spa|Casino',case=False),'place']='Entertainement'\n",
    "\n",
    "df_classified.loc[df_classified['place'].str.\\\n",
    "                  contains('soccer field|sport|gym|stadium|surf|pool|golf|\\baseball',\\\n",
    "                           case=False),'place']='Sport'\n",
    "\n",
    "#Change the name of a day to its type (work day or week end)\n",
    "df_classified.loc[df_classified['day'].str.contains('mon|tue|wed|thu|fri',case=False),'day']='Work day'\n",
    "df_classified.loc[df_classified['day'].str.contains('sat|sun',case=False),'day']='Week end'\n",
    "\n",
    "#Keep only the place types we mentioned before\n",
    "df_classified=df_classified.loc[df_classified['place'].str.contains('sport|Entertainement|Work|Consume|Move|Home|Culture|Drink|Study|Eat',case=False),:]\n",
    "df_places=df_classified.groupby(['place','day'],as_index=False).agg({'country':'size'}).\\\n",
    "                sort_values(by='country',ascending=False)[['place','day','country']].\\\n",
    "                rename(columns={'country':'numb checkins','place':'place type','day':'day type'})\n",
    "#df1.groupby('place_x').count().sort_values(by='day_x',ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_classified.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_places.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we move to visualizing the distributions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=df_places['numb checkins']/df_places['numb checkins'].sum(),\\\n",
    "            y=df_places['place type'],orient='h',hue=df_places['day type'])\n",
    "plt.xlabel('probability')\n",
    "plt.ylabel('type of place')\n",
    "plt.title('meeting patterns between friends')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The visualization above shows us that people are the most likely to be studying. This observation can be explained by the fact that students are the most likely to use sociaal media, so the most number of checkins can be found among students.\n",
    "- However, we can't draw more conlusions because we have to take into account that there are 5 work days and 2 week end days in a week. $\\rightarrow$ In order to be able to compare and study people's checkins, we normalize by the number of days each probability.\n",
    "- We will :\n",
    "1) Divide each probability that any event occured in a working day by 5\n",
    "2) Divide each probability that any event occured in a week end day by 2\n",
    "3) Get the normalized difference (normalized probability that a user checks in in a working day - normalized probability that a user checks in in a week end day)\n",
    "4) Divide the result by the probability of occurence in a work day to have a ratio.\n",
    "The final equation we will have is (for each place):\n",
    "$$\n",
    "final ratio = \\frac{\\frac{P_{workday}}{5}-\\frac{P_{workend}}{2}}{\\frac{P_{weekday}}{5}}\n",
    "$$\n",
    "\n",
    "- In the end:\n",
    "1)If this difference is positive : people are more likely to checkin in the place in a working day\n",
    "2)If this difference is negative : people are more likely to checkin in the place in a week end day\n",
    "3)The absolute value gives us the magnitude of the absolute ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ratio (x):\n",
    "    \"\"\"\n",
    "    Function to get difference between items atn then normalize\n",
    "    Since x contains one negative (week end) and one positive (work day) value,\n",
    "    max(x) is the value  of probability for a working day\n",
    "    \"\"\"\n",
    "    return sum(x)/max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#df4=df_classified.copy()\n",
    "#normalize to have proba/day and then take the difference between the items:\n",
    "\n",
    "#First, we divide by the number of days and set week end probabilities to be negative values\n",
    "#So that we perform a sum after\n",
    "df_places.loc[df_places['day type'].str.contains('Week|end',case=False),'numb checkins']=\\\n",
    "                            -df_places.loc[df_places['day type'].str.\\\n",
    "                                               contains('Week|end',case=False),'numb checkins']/2\n",
    "\n",
    "df_places.loc[df_places['day type'].str.contains('Work|day',case=False),'numb checkins']=\\\n",
    "                            df_places.loc[df_places['day type'].str.\\\n",
    "                                              contains('Work|day',case=False),'numb checkins']/5\n",
    "#Now we groupby the place type and get the ratio we need\n",
    "df_places=df_places.groupby('place type',as_index=False).\\\n",
    "                            agg({'numb checkins':ratio},axis='columns').\\\n",
    "                            sort_values(by='numb checkins',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(x=df_places['numb checkins'],y=df_places['place type'],orient='h', palette='viridis_r')\n",
    "plt.xlabel('difference between normalized probabilities')\n",
    "plt.ylabel('type of place')\n",
    "plt.title('meeting patterns between friends day week vs week end')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4) Times checkin patterns**\n",
    "\n",
    "- In this final part, we will study the time friends meet.\n",
    "We willclassify a day in two parts : day and night.\n",
    "- Since work usually finished at 5pm, we consider day hours of the day between 5h and 17h, and night as other hours of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#In order to get our times, we substract 4 from total checkin hours\n",
    "#Then we get the hours that are superior to 14\n",
    "df_classified.loc[:,'night']=(df_classified.loc[:,'local time'].dt.hour-5)>12\n",
    "df_classified.loc[:,'day']=(df_classified.loc[:,'local time'].dt.hour-5)<=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_classified.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We groupby the place type and then get the ratio of evening checkins\n",
    "df_classified=df_classified.groupby('place',as_index=False).\\\n",
    "                      agg({'night':'sum','country':'size','day':'sum'}).\\\n",
    "                      rename(columns={'country':'numb checkins'})\n",
    "df_classified['ratio night']=df_classified['night']/sum(df_classified['numb checkins'])\n",
    "df_classified['ratio day']=df_classified['day']/sum(df_classified['numb checkins'])\n",
    "#We sort values to have a good looking visualization\n",
    "df_classified=df_classified.sort_values(by='ratio day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_classified.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_classified.plot(x='place', y=['ratio night','ratio day'], kind=\"barh\",figsize=(12,6),width=0.8)\n",
    "\n",
    "plt.xlabel('probiabilities')\n",
    "plt.ylabel('type of place')\n",
    "plt.title('checkins in different times of day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first notice that :\n",
    "1) The biggest probability of checking in with friends during day occurs during studying\n",
    "2) The biggest probability of checking in with friends during night occurs during night.\n",
    "- We now do the same work we did previously with the places patterns in order to compare the ratio of checkins:\n",
    "1) We get the difference between probabilities that a user checks in during the day or during night.\n",
    "2) We divide the result by the probability of a checkin during the day\n",
    "The final equation we will have is (for each place):\n",
    "$$\n",
    "final ratio = \\frac{P_{daycheckin}-P_{nightcheckin}}{P_{daycheckin}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#get the ratio ( values used in the plot below)\n",
    "df_classified['ratio final']=(df_classified['ratio day']-df_classified['ratio night'])/df_classified['ratio day']\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(x='ratio final',y='place',data=df_classified.sort_values(by='ratio final',ascending=False),\\\n",
    "            orient='h', palette='viridis_r')\n",
    "\n",
    "plt.xlabel('checkin patterns in different times of day')\n",
    "plt.ylabel('type of place')\n",
    "plt.title('ratio of probabilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that friends are more likely to meet during the day to study or work. On the other hand, they are more likely to meet at night to have night, even if the difference is small.\n",
    "- We can conlude that most checkins happen to be during the day and that people tend less to checkin at night"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) Conclusions :\n",
    "- People tend to meet their friends more in work or study places during the week. This can be explained by the fact that people usually have their coworkers and classmates as friends on social media. Studying or working is part of people's obligations and these are task are generally proceeded during the week\n",
    "- However, when it comes to free time (week end for most of people), people choose to meet their friends in diverting places (every other category that doesn't involve working or studying). Specifically, people are the most likely to go out in weekend to have drinks or to entertainement places.\n",
    "- Finally, people tend to spend their day working and studying, and then spend their evening and night in diverting places (eating, having drinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada-venv",
   "language": "python",
   "name": "ada-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}